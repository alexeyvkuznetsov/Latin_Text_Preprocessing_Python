{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Read-data-from-tsv-source\" data-toc-modified-id=\"Read-data-from-tsv-source-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Read data from tsv source</a></span></li><li><span><a href=\"#Connect-to-database\" data-toc-modified-id=\"Connect-to-database-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Connect to database</a></span></li><li><span><a href=\"#Save-subreddit-category-info\" data-toc-modified-id=\"Save-subreddit-category-info-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Save subreddit category info</a></span></li><li><span><a href=\"#Cleaning-function\" data-toc-modified-id=\"Cleaning-function-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Cleaning function</a></span></li><li><span><a href=\"#Create-new-column-in-dataframe\" data-toc-modified-id=\"Create-new-column-in-dataframe-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Create new column in dataframe</a></span></li><li><span><a href=\"#Load-spaCy\" data-toc-modified-id=\"Load-spaCy-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Load spaCy</a></span></li><li><span><a href=\"#Iterate-over-all-rows-and-perform-NLP\" data-toc-modified-id=\"Iterate-over-all-rows-and-perform-NLP-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Iterate over all rows and perform NLP</a></span></li><li><span><a href=\"#Check-results\" data-toc-modified-id=\"Check-results-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Check results</a></span></li><li><span><a href=\"#Save-to-database\" data-toc-modified-id=\"Save-to-database-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Save to database</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from tsv source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T18:25:50.316246Z",
     "start_time": "2018-12-11T18:25:37.226960Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"rspct.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect('selfposts.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save subreddit category info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"subreddit_info.csv\").to_sql(\"categories\", con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(s):\n",
    "    s = s.replace(r'<lb>', \"\\n\")\n",
    "    s = s.replace(r'<tab>', \"\\i\")\n",
    "    s = re.sub(r'<br */*>', \"\\n\", s)\n",
    "    s = s.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"&amp;\", \"&\")\n",
    "    # markdown urls\n",
    "    s = re.sub(r'\\(https*://[^\\)]*\\)', \"\", s)\n",
    "    # normal urls\n",
    "    s = re.sub(r'https*://[^\\s]*', \"\", s)\n",
    "    s = re.sub(r'_+', ' ', s)\n",
    "    s = re.sub(r'\"+', '\"', s)\n",
    "    return str(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new column in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"selftext_clean\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    df.at[i, \"selftext_clean\"] = clean(row.selftext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over all rows and perform NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if(row[\"selftext_clean\"] and len(str(row[\"selftext_clean\"])) < 1000000):\n",
    "        doc = nlp(str(row[\"selftext_clean\"]))\n",
    "        adjectives = []\n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        lemmas = []\n",
    "\n",
    "        for token in doc:\n",
    "            lemmas.append(token.lemma_)\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_)\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "                nouns.append(token.lemma_)\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_)\n",
    "                \n",
    "        df.at[i, \"selftext_lemma\"] = \" \".join(lemmas)                \n",
    "        df.at[i, \"selftext_nouns\"] = \" \".join(nouns)\n",
    "        df.at[i, \"selftext_adjectives\"] = \" \".join(adjectives)\n",
    "        df.at[i, \"selftext_verbs\"] = \" \".join(verbs)\n",
    "        df.at[i, \"selftext_nav\"] = \" \".join(nouns+adjectives+verbs)\n",
    "        df.at[i, \"no_tokens\"] = len(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('posts_nlp', con)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
