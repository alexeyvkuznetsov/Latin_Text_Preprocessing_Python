{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stem.latin.j_v import JVReplacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup CLTK tools\n",
    "\n",
    "word_tokenizer = WordTokenizer('latin')\n",
    "replacer = JVReplacer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of words\n",
    "We can use the Latin Library to generate a list of possible Latin words to match acrostics against by:\n",
    "- Getting the raw text of the Latin Library\n",
    "- Preproccessing the text to remove numbers, punctuation, English words, etc.\n",
    "- Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get raw text of the Latin Library\n",
    "#\n",
    "# Note that the CLTK Latin Library was updated on 3/25/17\n",
    "# to fix line breaks in some of the hexameter poems included\n",
    "# in this experiment. Please delete and reimport the\n",
    "# CLTK Latin Library corpus to follow along.\n",
    "\n",
    "ll_raw = latinlibrary.raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "def preprocess(text):    \n",
    "\n",
    "    text = re.sub(r'&aelig;','ae',text)\n",
    "    text = re.sub(r'&AElig;','AE',text)\n",
    "    text = re.sub(r'&oelig;','oe',text)\n",
    "    text = re.sub(r'&OElig;','OE',text)\n",
    "    \n",
    "    text = re.sub('\\x00',' ',text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = replacer.replace(text)\n",
    "    \n",
    "\n",
    "    text= re.sub(r'&lt;','<',text)\n",
    "    text= re.sub(r'&gt;','>',text)    \n",
    "    \n",
    "    punctuation =\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    remove_list = [r'\\bthe latin library\\b',\n",
    "                   r'\\bthe classics page\\b',\n",
    "                   r'\\bneo-latin\\b', \n",
    "                   r'\\bmedieval latin\\b',\n",
    "                   r'\\bchristian latin\\b',\n",
    "                   r'\\bthe miscellany\\b'\n",
    "                  ]\n",
    "\n",
    "    for pattern in remove_list:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    text = re.sub('[ ]+',' ', text) # Remove double spaces\n",
    "    text = re.sub('\\s+\\n+\\s+','\\n', text) # Remove double lines and trim spaces around new lines\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess Latin Library\n",
    "\n",
    "ll_text = preprocess(ll_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the preprocessed text on white space; no need for enclitic splitting, etc. here\n",
    "\n",
    "ll_tokens = ll_text.split()\n",
    "\n",
    "# Remove tokens less than 3 letters long\n",
    "ll_tokens = [token for token in ll_tokens if len(token) > 2]\n",
    "\n",
    "# Remove tokens made up of a single character, e.g. 'aaaa'\n",
    "ll_tokens = [token for token in ll_tokens if token != len(token) * token[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find palindromes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to test for palindromes\n",
    "\n",
    "def is_palindrome(token):\n",
    "    return token == token[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter tokens for palindromes\n",
    "\n",
    "palindromes = [token for token in ll_tokens if is_palindrome(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('non', 166062), ('esse', 49426), ('illi', 9921), ('ibi', 7153), ('ecce', 3661), ('tot', 3444), ('sumus', 2678), ('sis', 1526), ('usu', 1471), ('tenet', 1072)]\n"
     ]
    }
   ],
   "source": [
    "# List the 10 most frequent palindromes\n",
    "\n",
    "c = Counter(palindromes)\n",
    "print(c.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a list of the longest palindromes \n",
    "\n",
    "# Keep only tokens that appear at least 3 times\n",
    "c = Counter(palindromes)\n",
    "palindromes = [k for k, c in c.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n",
      "['massinissam', 'simillimis', 'sumeremus', 'sarabaras', 'muratarum', 'siluulis', 'aballaba', 'suillius', 'sumamus', 'sumimus']\n"
     ]
    }
   ],
   "source": [
    "palindromes.sort(key = len, reverse=True)\n",
    "print(len(palindromes))\n",
    "print(palindromes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['massinissam', 'simillimis', 'sumeremus', 'sarabaras', 'muratarum', 'siluulis', 'aballaba', 'suillius', 'sumamus', 'sumimus', 'taedeat', 'apocopa', 'murorum', 'senones', 'tereret', 'mutatum', 'matutam', 'rotator', 'ccciccc', 'sumemus', 'erexere', 'eregere', 'erepere', 'merorem', 'nomimon', 'madidam', 'sububus', 'tingnit', 'sinonis', 'siccis', 'messem', 'massam', 'terret', 'succus', 'summus', 'marram', 'saccas', 'soccos', 'mammam', 'selles', 'iessei', 'mannam', 'mappam', 'tinnit', 'iussui', 'mullum', 'maiiam', 'murrum', 'senes', 'sumus', 'solos', 'sitis', 'sedes', 'malam', 'tenet', 'inani', 'seges', 'tepet', 'murum', 'etate', 'mutum', 'sonos', 'teret', 'temet', 'sucus', 'siuis', 'mulum', 'cxxxc', 'refer', 'seres', 'aenea', 'sudus', 'aerea', 'anona', 'silis', 'sinis', 'sanas', 'satas', 'reuer', 'soros', 'ogygo', 'sefes', 'siris', 'simis', 'sepes', 'eabae', 'iadai', 'ianai', 'atita', 'aziza', 'adeda', 'teget', 'neuen', 'taxat', 'seces', 'sagas', 'rogor', 'egoge', 'sicis', 'ccicc', 'subus', 'eumue', 'tioit', 'essse', 'sacas', 'illli', 'egage', 'lunul', 'nomon', 'maiam', 'itati', 'robor', 'ebibe', 'rotor', 'saxas', 'cabac', 'obibo', 'minim', 'suius', 'inoni', 'ababa', 'noson', 'aitia', 'surus', 'oeteo', 'acuca', 'noton', 'seses', 'adada', 'tedet', 'aeaea', 'amoma', 'aegea', 'tabat', 'agaga', '\\uf8ffnon\\uf8ff', 'cilic', 'susus', 'neten', 'suus', 'illi', 'esse', 'ecce', 'otto', 'teet', 'abba', 'alla', 'assa', 'anna', 'acca', 'arra', 'adda', 'emme', 'amma', 'xiix', 'cxxc', 'issi', 'bppb', 'cqqc', 'elle', 'atta', 'noon', 'siis', 'iffi', 'icci', 'oddo', 'iuui', 'appa', 'irri', 'ollo', 'urru', 'non', 'usu', 'ibi', 'xix', 'tot', 'sis', 'oro', 'ouo', 'imi', 'ara', 'ere', 'eae', 'ede', 'ama', 'tit', 'iri', 'ala', 'ini', 'oto', 'odo', 'sus', 'bab', 'nun', 'fyf', 'cic', 'rer', 'mam', 'ese', 'eme', 'ndn', 'ada', 'gog', 'mum', 'ana', 'aia', 'asa', 'aza', 'ono', 'aua', 'uau', 'mem', 'tet', 'ili', 'nhn', 'eie', 'pop', 'pup', 'ses', 'kak', 'a˝a', 'tat', 'sas', 'sos', 'ici', 'bob', 'uiu', 'oxo', 'ioi', 'exe', 'ihi', 'ene', 'uou', 'hoh', 'idi', 'eue', 'ele', 'nan', 'did', 'ata', 'νῦν', 'nin', 'aba', 'ded', 'νον', 'gcg', 'nyn', 'iui', 'cxc', 'iei', 'olo', 'lol', 'ete', 'ror', 'scs', 'omo', 'pap', 'ydy', 'fuf', 'ege', 'bib', 'tut', 'aea', 'isi', 'afa', 'aha', 'coc', 'cac', 'geg', 'aga', 'opo', 'umu', 'ipi', 'rar', 'epe', 'nen', 'ixi', 'unu', 'igi']\n"
     ]
    }
   ],
   "source": [
    "print(palindromes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multiple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for combining list elements into various length strings\n",
    "\n",
    "def find_ngrams(input_list, n, separator=\" \"):\n",
    "    temp = list(zip(*[input_list[i:] for i in range(n)]))\n",
    "    ngrams = [separator.join(t) for t in temp]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'esse non esse': 13, 'non esse non': 11, 'non sedes non': 2, 'non sis non': 1, 'sumus non sumus': 1, 'non sumus non': 1, 'ccciccc ccciccc ccciccc': 1, 'cic cic cic': 1, 'non tenet non': 1, 'aut non tua': 1, 'ibi esse ibi': 1, 'esse ibi esse': 1})\n"
     ]
    }
   ],
   "source": [
    "test = find_ngrams(ll_tokens, 3)\n",
    "palinwords = [token for token in test if is_palindrome(token)]\n",
    "print(Counter(palinwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "test = find_ngrams(ll_tokens, 4)\n",
    "palinwords = [token for token in test if is_palindrome(token)]\n",
    "print(Counter(palinwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "test = find_ngrams(ll_tokens, 5)\n",
    "palinwords = [token for token in test if is_palindrome(token)]\n",
    "print(Counter(palinwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "test = find_ngrams(ll_tokens, 6)\n",
    "palinwords = [token for token in test if is_palindrome(token)]\n",
    "print(Counter(palinwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "test = find_ngrams(ll_tokens, 7)\n",
    "palinwords = [token for token in test if is_palindrome(token)]\n",
    "print(Counter(palinwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
